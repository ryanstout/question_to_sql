- [ ] snowflake async stack trace fix https://github.com/hightouchio/snowflake-connector-nodejs/blob/e059b8cb897891a12e30b078faa327bd2282f2f5/lib/http/base.js#L98
- [ ] use pdm instead of peotry? https://pdm.fming.dev/latest/

# UI

- [ ] remove name from signup form

# Data model

- [ ] https://www.npmjs.com/package/prisma-erd-generator

# Packages

- [ ] logging
  - [ ] Structured logging when an object is passed as a second argument
  - [ ] maybe better logging package https://github.com/pimterry/loglevel/issues/170
  - [ ] extract LOG_LEVEL as a plugin https://github.com/vectrlabs/loglevel-debug/blob/master/index.js
  - [ ] colorized log output? https://github.com/kentcdodds/loglevel-colored-level-**prefix**

# Docker

- [ ] add git sha to build env https://artsy.github.io/blog/2018/09/10/Dockerhub-Stamping-Commits/ and https://community.fly.io/t/git-commit-sha-of-current-build/1870
- [ ] Fix python release in sentry
- [ ] we should not have to add prop-type to the package, but without it nivo is failing
- [ ] can we only deploy the git version, not the local?
- [ ] should add sentry versioning https://docs.sentry.io/platforms/javascript/guides/remix/sourcemaps/

# Devprod

- [ ] use localhost redirects https://www.npmjs.com/package/chalet
- [ ] shell formatting
- [ ] Maybe use trunk?
- [ ] sync line size across everything https://github.com/ryanstout/nlpquery/commit/56872f3eaca35722b4398a47c7e3dd2367d7ee11#r100412156

# Open Source

- [ ] add 0 to node docs https://github.com/nodejs/node/issues/28457

# Remix

- [ ] https://github.com/kiliman/rmx-cli
- [ ] https://github.com/yesmeck/remix-routes
- [ ] array in zod https://discord.com/channels/893487829802418277/893488038477434881/1074757486470516836

# Javascript Lang

- [ ] force const use https://eslint.org/docs/latest/rules/prefer-const#resources
- [ ] https://www.npmjs.com/package/sort-package-json
- [ ] https://github.com/trivago/prettier-plugin-sort-imports/issues/112 is broken and does not support ignores should use https://www.npmjs.com/package/prettier-plugin-organize-imports?activeTab=versions instead.
- [ ] npm alternative? https://pnpm.io/installation
- [ ] cleaner stack traces with relative paths? https://github.com/tapjs/stack-utils

# Javascript Infrastructure

- [ ] implement timeouts across the stack, including fetches

# Prisma

- [ ] https://www.prisma.io/docs/concepts/components/prisma-client/working-with-prismaclient/logging
- [ ] add findfirstorthrow to python library

# Javascript Packages

- `export default this` in index on ramda

# Github actions

- [ ] auto merge node updates https://docs.github.com/en/code-security/dependabot/working-with-dependabot/automating-dependabot-with-github-actions#fetch-metadata-about-a-pull-request
- [ ] integrate code coverage into CI, use c8's formatting to do this
- [ ] https://github.com/actions/setup-node/issues/531 looks like we could use corepack to setup specific versions
- [ ] how do the PR annotations work? Can we get pylint working like tsc? https://github.com/marketplace/actions/pylint-pr-annotator
- [ ] https://github.com/tj-actions/depcheck/blob/main/entrypoint.sh needs to not pass the ignore flag if nothing is set

# Infrastructure

- [ ] https://resend.com/secret?utm_source=waitlist_valid

# AI

- https://cohere.ai/generate
- Cohere, Anthropic & OneA
- https://twitter.com/ompemi/status/1628077623183126532

# Python General

- [ ] setup sentry on cli and other scripts `configure_sentry`
- [ ] in prod rich traceback is getting installed somehow... `rg '\binstall\(' --no-ignore-vcs`
- [ ] Stack trace on sentry seems to be limited
- [ ] Sentry is not enabled locally, should use stdout DSN

# Python Lang

- [ ] explicitly version pip
- [ ] list comprehensions are messy https://github.com/psf/black/issues/2121
- [ ] functional https://github.com/EntilZha/PyFunctional
- [ ] another runtime type checking option https://docs.pydantic.dev
- [ ] typeguard for runtime type checking https://github.com/agronholm/typeguard
- [ ] automatically remove unused imports https://github.com/PyCQA/isort/issues/1105
- [ ] yeah agreed that pickle is fine, just confused as to why JSON didn't handle it correctly if it's all getting sent to the FE as JSON in the end :thinking_face: I'm guessing flask has a better default encoder? I have it on my personal TODO to investigate.
- [ ] record mode in vcr doesn't work the same if run globally vs locally on a specific file

# Python Testing

- [ ] take over pytest vcr plugin

# Python VS Code

- Why isn't pylint working on all files in vs code?

# Node VS Code

- [ ]

# Snowflake

- [ ] should get table count all in one query using INFORMATION_SCEMA
- [ ] should avoid running queries on tables with a ton of data

---

questions = all_questions_marked_as_correct

for question in questions:
result = rerun_request(question.userSql)

# compare values in result

assert_results_match_without_while_being_column_agnostic(
result,
question.resultdata
)

--- open ai stack trace

```
raceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/Users/mike/Projects/nlpquery/.venv/lib/python3.10/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/Users/mike/Projects/nlpquery/.venv/lib/python3.10/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/Users/mike/Projects/nlpquery/.venv/lib/python3.10/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/Users/mike/Projects/nlpquery/.venv/lib/python3.10/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/Users/mike/Projects/nlpquery/.venv/lib/python3.10/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/Users/mike/Projects/nlpquery/python/cli.py", line 141, in import_datasource
    Importer(**kwargs)
  File "/Users/mike/Projects/nlpquery/python/importer.py", line 109, in __init__
    self.create_table_records(data_source)
  File "/Users/mike/Projects/nlpquery/python/importer.py", line 145, in create_table_records
    self.create_table_record(data_source, table)
  File "/Users/mike/Projects/nlpquery/python/importer.py", line 168, in create_table_record
    self.embedding_builder.add_table(
  File "/Users/mike/Projects/nlpquery/python/embeddings/embedding_builder.py", line 133, in add_table
    result.get()
  File "/Users/mike/.asdf/installs/python/3.10.9/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/pool.py", line 774, in get
    raise self._value
  File "/Users/mike/.asdf/installs/python/3.10.9/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/Users/mike/Projects/nlpquery/python/embeddings/embedding_builder.py", line 205, in add_table_column_values

  File "/Users/mike/Projects/nlpquery/python/embeddings/ann_index.py", line 37, in add
    embedding = Embedding(content, embedder=OpenAIEmbedder)
  File "/Users/mike/Projects/nlpquery/python/embeddings/embedding.py", line 41, in __init__
    embedding: np.ndarray = self.embedder.encode(content_str)
  File "/Users/mike/Projects/nlpquery/python/embeddings/openai_embedder.py", line 12, in encode
    result = openai_throttled.embed(model="text-embedding-ada-002", input=content_str)
  File "/Users/mike/Projects/nlpquery/python/utils/openai_rate_throttled.py", line 113, in embed
    result = openai.Embedding.create(input=input, **kwargs)
  File "/Users/mike/Projects/nlpquery/.venv/lib/python3.10/site-packages/openai/api_resources/embedding.py", line 33, in create
    response = super().create(*args, **kwargs)
  File "/Users/mike/Projects/nlpquery/.venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
  File "/Users/mike/Projects/nlpquery/.venv/lib/python3.10/site-packages/openai/api_requestor.py", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)
  File "/Users/mike/Projects/nlpquery/.venv/lib/python3.10/site-packages/openai/api_requestor.py", line 619, in _interpret_response
    self._interpret_response_line(
  File "/Users/mike/Projects/nlpquery/.venv/lib/python3.10/site-packages/openai/api_requestor.py", line 679, in _interpret_response_line
    raise self.handle_error_response(
openai.error.RateLimitError: The server is currently overloaded with other requests. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists.
```
