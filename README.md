# NLP Query

A BI tool that lets you dig into your data using natural language

# Development

All local installation is documented in:

```shell
bin/setup.sh
npm run dev
```

# Deployment

Attach the python app to the database:

```shell
fly pg attach knolbe-db --config python.toml --database-name knolbe
```

Proxy to the prod DB for inspection:

```shell
fly proxy 5431:5432 -a knolbe-db
```

## [Docker](docker/readme.md)

# Prisma

Generate the python and ts schemas with:
`prisma generate --schema prisma/schema.prisma`

# Frontend

- All route logic is very thin: business logic should be pushed into `~/lib/`, view logic should validate params, redirect, hit lib logic, and then pass props to components for rendering.
- All `~/components` should not use "top-level state"
- All `action` and `loader` exports should be thin and pass to a `~/lib/*.server` file (this is the "frontend of the backend")
- `action` and `loader` exports should do all input validation and state retrieval before passing to the `*.server` functions

# Python API

Start the server:

```shell
poetry run python python/server.py
```

Ask a question:

```shell
http POST http://127.0.0.1:5000/ 'question:="my question"' data_source_id:=1
```

Get results:

```shell
http POST http://0.0.0.0:5000/query data_source_id:=1 'sql:="SELECT * FROM CUSTOMER LIMIT 10"'
```

## CLI Tools

First, we need to import the table data:

```shell
poetry run nlp \
  --user-id=1 \
  --database-name=FIVETRAN_DATABASE \
  --table-limit=100 \
  --column-limit=1000 \
  --column-value-limit=1000 \
  | tee user.log
```

Once everything is imported, we can ask questions:

```shell
poetry run python python/answer.py \
  --data-source-id 1 \
  --question "What are the top orders of all time?"
```

# Schemas [WIP]

Place your test database in a local postgres (currenthly hardcoded to db name of witharsenal_prod_copy)

Generate the schema with:
`ts-node --require tsconfig-paths/register  --transpileOnly app/models/generate_schema.ts > prisma/test_schema.sql`

# --------

# Tables

# --------

# User

# QuestionGroup

has_many :questions
correctSql - the sql that correctly answers the question (updated when marked as correct)

# Question

question - the english question
userSql - the sql edited by the user
codexSql - the sql generated by codex model
correctState - null, 0 = incorrect, 1 = correct

# TODO:

translate DATE_TRUNC('month', created_at) to

Test questions:
How many people who bought an Arsenal 2 Pro also bought a phone mount?
How many people who bought a sd card also bought a phone mount?
What percent of orders use each shipping method?
How many pacific northwest customers spent more than $160?
How many Arsenal 2 Pro's have been sold each month?
What are the fields in the orders table?
What credit card is used most in Germany?
For the shipments that failed, how much money was spent on their associated order?
What products have been shipped to ryanstout@gmail.com?
What are the top selling products each december?
What percent of orders that bought a phone mount spent more than $300?

Future support:
Sumarize some text field...

# Database

## Snowflake

- API responses do not return raw SQL, everything comes back as a JSON object
  - Unsure if this is just the specific API we are using or if it is a limitation of Snowflake
- You'll get a `Table 'ABANDONED_CHECKOUTS' does not exist or not authorized.` if you don't specify a specific schema when

# Connections

## Shopify

- [Staff role](https://help.shopify.com/en/manual/your-account/staff-accounts/staff-permissions/staff-permissions-descriptions#apps-and-channels-permissions) does not allow us to create API keys. API keys must be created by generating a private app, which requires a specific permissions set. [More info.](https://help.plytix.com/en/getting-api-credentials-from-your-shopify-store)
