datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

generator client {
  provider = "prisma-client-js"
}

generator pyclient {
  provider             = "prisma-client-py"
  interface            = "sync"
  recursive_type_depth = 5
}

model User {
  id    Int    @id @default(autoincrement())
  email String @unique
  name  String

  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt

  password       Password?
  questionGroups QuestionGroup[]
  Question       Question[]

  business   Business? @relation(fields: [businessId], references: [id])
  businessId Int?
}

model Password {
  hash String

  user   User @relation(fields: [userId], references: [id], onDelete: Cascade, onUpdate: Cascade)
  userId Int  @unique
}

model QuestionGroup {
  id Int @id @default(autoincrement())

  user   User @relation(fields: [userId], references: [id], onDelete: Cascade, onUpdate: Cascade)
  userId Int

  correctSql String?

  Question Question[]
}

enum FeedbackState {
  INVALID
  UNANSWERED
  UNKNOWN
  CORRECT
  INCORRECT
}

// TODO should have openai state, since it can fail on that end
model Question {
  id       Int    @id @default(autoincrement())
  question String

  userSql  String // the query, starts with the codex model output, then can be updated by the user
  codexSql String // the original query generated from openai

  // should be unset initially, and result is only stored if the
  feedbackState FeedbackState @default(UNANSWERED)
  resultData    Json?

  dataSourceId Int?
  dataSource   DataSource? @relation(fields: [dataSourceId], references: [id])

  user   User @relation(fields: [userId], references: [id], onDelete: Cascade, onUpdate: Cascade)
  userId Int

  questionGroup   QuestionGroup @relation(fields: [questionGroupId], references: [id], onDelete: Cascade, onUpdate: Cascade)
  questionGroupId Int

  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt

  @@unique([id, userId, questionGroupId])
}

model Business {
  id   Int    @id @default(autoincrement())
  name String

  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt

  users   User[]
  sources DataSource[]
}

enum DataSourceType {
  SNOWFLAKE
  POSTGRES
}

model DataSource {
  id Int @id @default(autoincrement())

  // for user to identify the source
  name        String
  credentials Json
  type        DataSourceType

  business   Business? @relation(fields: [businessId], references: [id])
  businessId Int?

  dataSourceTableDescriptions DataSourceTableDescription[]
  EmbeddingLink               EmbeddingLink[]
  Question                    Question[]
}

model DataSourceTableDescription {
  id Int @id @default(autoincrement())

  DataSource   DataSource? @relation(fields: [dataSourceId], references: [id])
  dataSourceId Int?

  // manually allow the user to skip this table
  skip Boolean

  // store the FQN and use a application-level function to extract the target
  fullyQualifiedName String

  columns DataSourceTableColumn[]

  generatedSQLCache String

  // TODO can we enforce JSON schema inside this JSONB array
  // we could generate a bunch of different embedding and include as a JSON array
  embeddingsCache Json

  embeddingLinks EmbeddingLink[]

  @@unique([dataSourceId, fullyQualifiedName])
}

model DataSourceTableColumn {
  id Int @id @default(autoincrement())

  skip Boolean

  // place to store distinct number of entries, etc and other analysis
  inspectionMetadata Json

  // core column description to be used to generate a SQL describe later on
  name    String
  type    String
  kind    String
  isNull  Boolean
  default String

  // used in cardinality calculations
  distinctRows Int
  rows         Int

  extendedProperties Json

  // unsure *exactly* what the DB structure should look like here, so we can stuff
  // anything else we need in the extended properties, for example:
  // "primary key" String
  // "unique key" String
  // check      any
  // expression any
  // comment    any
  // "policy name": any

  embeddingsCache              Json
  DataSourceTableDescription   DataSourceTableDescription? @relation(fields: [dataSourceTableDescriptionId], references: [id])
  dataSourceTableDescriptionId Int?

  embeddingLinks EmbeddingLink[]
}

// When we query the embeddings stored in a Faiss index, the result is a list
// of indexes into this table. This table points to the table and/or column.
// NULL's are used if it only points to one or the other.
model EmbeddingLink {
  id Int @id @default(autoincrement())

  DataSource   DataSource? @relation(fields: [dataSourceId], references: [id])
  dataSourceId Int?

  // Which index is this (see docs/prompt_embeddings.md)
  indexNumber Int

  // What is the position in the embedding matrix
  indexOffset Int

  // Hash of the string used to generate the embedding (so we can cache them)
  contentHash String

  table   DataSourceTableDescription? @relation(fields: [tableId], references: [id])
  tableId Int?

  column   DataSourceTableColumn? @relation(fields: [columnId], references: [id])
  columnId Int?

  // Store a copy (for now) of the cell value so we can construct the Value Hints
  value String?

  // The main way we do lookups (from faiss index)
  @@index([dataSourceId, indexNumber, indexOffset], name: "faissIndex")
  // When rebuilding, we'll want to look up
  @@index([contentHash, tableId, columnId])
}

// Simple cache for contentHash -> embedding so we don't have to rehit the openai api
model EmbeddingCache {
  id Int @id @default(autoincrement())

  // Hash of the string used to generate the embedding (so we can cache them)
  contentHash String

  // The embedding in npz format
  embedding Bytes
}
